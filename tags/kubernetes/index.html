<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>kubernetes &middot; Fabrice Aneche</title>

    <meta name="description" content="">

    <meta name="generator" content="Hugo 0.55.3" />
    <meta name="twitter:card" content="summary">
    
    <meta name="twitter:title" content="kubernetes &middot; Fabrice Aneche">
    <meta name="twitter:description" content="">

    <meta property="og:type" content="article">
    <meta property="og:title" content="kubernetes &middot; Fabrice Aneche">
    <meta property="og:description" content="">

    <link href='//fonts.googleapis.com/css?family=Source+Sans+Pro:400,700|Oxygen:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/pure/0.6.0/pure-min.css">
    <!--[if lte IE 8]>
        <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/pure/0.6.0/grids-responsive-old-ie-min.css">
    <![endif]-->
    <!--[if gt IE 8]><!-->
        <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/pure/0.6.0/grids-responsive-min.css">
    <!--<![endif]-->

    <link rel="stylesheet" href="https://blog.nobugware.com/css/all.min.css">
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet">

    <link rel="alternate" type="application/rss+xml" title="Fabrice Aneche" href="https://blog.nobugware.com/index.xml" />
    <link href="https://blog.nobugware.com/css/prism.css" rel="stylesheet" />
</head>
<body>


<div id="layout" class="pure-g">
    <div class="sidebar pure-u-1 pure-u-md-1-4">
    <div class="header">
        <hgroup>
            <h1 class="brand-title"><a href="https://blog.nobugware.com">Fabrice Aneche</a></h1>
            <h2 class="brand-tagline"></h2>
        </hgroup>

        <nav class="nav">
            <ul class="nav-list">
                
                <li class="nav-item">
                    <a class="pure-button" href="http://www.nobugware.com"><i class="fa fa-building-o"></i> Hiring</a>
                </li>
                
                
                <li class="nav-item">
                    <a class="pure-button" href="https://twitter.com/akhenakh"><i class="fa fa-twitter"></i> Twitter</a>
                </li>
                
                
                <li class="nav-item">
                    <a class="pure-button" href="https://github.com/akhenakh "><i class="fa fa-github-alt"></i> github</a>
                </li>
                
                <li class="nav-item">
                    <a class="pure-button" href="https://blog.nobugware.com/index.xml"><i class="fa fa-rss"></i> rss</a>
                </li>
            </ul>
        </nav>
    </div>
</div>


    <div class="content pure-u-1 pure-u-md-3-4">
        <div>
            
            <div class="posts">
                
                <h1 class="content-subhead">30 May 2019, 00:00</h1>
                <section class="post">
                    <header class="post-header">

                        <a href="https://blog.nobugware.com/post/2019/k3s-on-arm64/" class="post-title">k3s on arm64</a>

                        <p class="post-meta">
                            
                            
                                under 
                                
                                <a class="post-category post-category-devops" href="https://blog.nobugware.com/categories/devops">devops</a>
                            
                        </p>
                    </header>

                    <div class="post-description">
                        <p>I&rsquo;m evaluating <a href="https://k3s.io/">k3s</a> a Lightweight Kubernetes on a <a href="http://wiki.pine64.org/index.php/ROCK64_Main_Page">rock64</a> (RK3328 Quad arm64).</p>

<p>At the time of writing the stable release is k3s <a href="https://github.com/rancher/k3s/releases/tag/v0.4.0">v0.4.0</a></p>

<p>Here are my notes:</p>

<ul>
<li>If you haven&rsquo;t installed k3s with the <code>install.sh</code>, you may need to load some modules: <code>br_netfilter</code> and <code>overlay</code></li>
<li>Docker is not needed since k3s is using containerd but it seems I had to start docker to initialized the whole cgroups, at least on Arch</li>
<li>For the Metrics API server to work you&rsquo;ll need to start k3s with those args <code>k3s server --kubelet-arg=&quot;address=0.0.0.0&quot;</code></li>

<li><p>Remember to change all the templates you nay need to arm64</p>

<pre><code>  containers:
  - name: kubernetes-dashboard
    image: k8s.gcr.io/kubernetes-dashboard-arm64:v1.10.1
</code></pre></li>

<li><p>Deploy metrics server
Clone the k3s and change the image to be arm64 <code>k8s.gcr.io/metrics-server-arm64:v0.3.1</code></p>

<p>git clone git@github.com:rancher/k3s.git
  vi k3s/recipes/metrics-server/metrics-server-deployment.yaml
  sudo  k3s kubectl -n kube-system create -f k3s/recipes/metrics-server</p></li>

<li><p>Deploy dashboard
Change the image to be arm64  <code>k8s.gcr.io/kubernetes-dashboard-arm64:v1.10.1</code>.
Add the skip option to the args <code>- --enable-skip-login</code></p>

<p>curl -sfL <a href="https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml">https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml</a> &gt; kubernetes-dashboard.yaml
  vi kubernetes-dashboard.yaml
  cat &lt;&lt;EOF | k3s kubectl create -f -
  apiVersion: rbac.authorization.k8s.io/v1beta1
  kind: ClusterRoleBinding
  metadata:
    name: kubernetes-dashboard
    labels:
      k8s-app: kubernetes-dashboard
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: cluster-admin
  subjects:</p>

<ul>
<li>kind: ServiceAccount
name: kubernetes-dashboard
namespace: kube-system
EOF
sudo  k3s kubectl -n kube-system create -f kubernetes-dashboard.yaml</li>
</ul></li>
</ul>

<p>Merge <code>/etc/rancher/k3s/k3s.yaml</code> to your <code>.kube/config</code> on your workstation host, then <code>kubectl proxy</code> to <a href="http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/#!/overview?namespace=default">localhost:8001</a></p>

                    </div>
                </section>
                
                <h1 class="content-subhead">27 Apr 2019, 02:01</h1>
                <section class="post">
                    <header class="post-header">

                        <a href="https://blog.nobugware.com/post/2019/deploying-a-website-with-caddy-git-and-kubernetes/" class="post-title">Deploying a website with Caddy, Git and Kubernetes</a>

                        <p class="post-meta">
                            
                            
                                under 
                                
                                <a class="post-category post-category-DevOps" href="https://blog.nobugware.com/categories/devops">DevOps</a>
                            
                        </p>
                    </header>

                    <div class="post-description">
                        

<p><a href="https://caddyserver.com">Caddy</a> is the swiss army of the web server, and with the <a href="https://caddyserver.com/blog/announcing-caddy-1_0-caddy-2-caddy-enterprise">recent commercial license changes</a>, it&rsquo;s time to give it some love back.</p>

<p>I have several static websites, some generated with <a href="https://gohugo.io">Hugo</a>, some are plain HTML.<br />
I wanted a small container, to run it inside a Kubernetes cluster, capable of pulling some git repos and serve them.</p>

<h2 id="caddy-git">Caddy-git</h2>

<p>Caddy is already capable of that with the help of <a href="https://github.com/abiosoft/caddy-git">caddy-git</a> unfortunately it is only working with ssh keys.<br />
I wanted it to use <a href="https://help.github.com/en/articles/creating-a-personal-access-token-for-the-command-line">Github access token</a>, also the current implementation is relying on the <code>git</code> command and <code>sh</code>, I wanted mine to be able to run on <a href="https://github.com/GoogleContainerTools/distroless">Distroless</a>.</p>

<h2 id="minigit">Minigit</h2>

<p>I&rsquo;ve used <a href="https://github.com/src-d/go-git">go-git</a> a pure Go implementation of git, to first make a clone of the <code>git</code> command: <a href="https://github.com/akhenakh/minigit">minigit</a>.<br />
<a href="https://github.com/akhenakh/minigit">minigit</a> can be useful in devops environnements and scriptings to facilitate git pulls.<br />
Faking the <code>git</code> command with <a href="https://github.com/akhenakh/minigit/">minigit</a> into your image and tweak caddy-git to pass an extra parameter <code>--ghtoken</code></p>

<pre><code>root /public
git https://github.com/myuser/repo {
   path /public
   clone_args --ghtoken XXXXXXXXXXXXX
   pull_args --ghtoken XXXXXXXXXXXXX
   interval 3600
}
</code></pre>

<p>It&rsquo;s nice but I wanted something cleaner and get rid of the <code>sh</code> dependency, I had to fork caddy-git.</p>

<h2 id="caddy-puregit">Caddy-puregit</h2>

<p>So here is caddy-puregit, a fork without execs but native pure Go git calls.<br />
Give it your token and it will clone then pull on regular intervals.</p>

<pre><code>root /public
puregit https://github.com/myuser/repo {
   path /public
   auth_token XXXXXXXXXXXXX 
   interval 3600
}
</code></pre>

<p>I&rsquo;ve also created a <a href="https://cloud.docker.com/repository/docker/akhenakh/caddy-hugo">Caddy + Hugo image</a>, so you can trigger a Hugo build on every commits.</p>

<pre><code>root /public
puregit https://github.com/myuser/hugo-blog {
   path /data
   then hugo --destination=/public --source=/data
   auth_token XXXXXXXXXXXXX 
   interval 3600
}
</code></pre>

<p>Here is <a href="https://github.com/akhenakh/caddy-puregit">caddy-puregit</a> and associated <a href="https://cloud.docker.com/u/akhenakh/repository/docker/akhenakh/caddy">Docker image</a> &amp; <a href="https://github.com/akhenakh/goimages/blob/master/caddy/Dockerfile">Dockerfile</a></p>

<h2 id="kubernetize">Kubernetize</h2>

<p>Since Caddy supports environment variables it&rsquo;s easy to deploy in k8s:</p>

<pre><code>root /public
puregit {$REPO} {
    auth_token {$TOKEN}
}
</code></pre>

<p>Put your token into a secret and expose it as an environment variable.</p>

<p><a href="https://gist.github.com/akhenakh/f3836dc3cf6ca06a003c562dde43ff07">Here is a template</a> which will deploy caddy and pull your repo then serving it according to the config.</p>

<h2 id="notes">Notes</h2>

<p>For development purpose, to work on a new Caddy plugin you can use the <a href="https://godoc.org/github.com/mholt/caddy/caddyhttp/httpserver#RegisterDevDirective">RegisterDevDirective</a>, or you have to fork Caddy.</p>

<p>I don&rsquo;t plan on maintaining this fork but I&rsquo;ll reach out to the author since a pure Go git concept is working maybe he will be interested.</p>

                    </div>
                </section>
                
                <h1 class="content-subhead">11 Mar 2019, 00:32</h1>
                <section class="post">
                    <header class="post-header">

                        <a href="https://blog.nobugware.com/post/2019/kubernetes_mesh_network_load_balancing_grpc_services/" class="post-title">gRPC Load Balancing inside Kubernetes</a>

                        <p class="post-meta">
                            
                            
                                under 
                                
                                <a class="post-category post-category-Devops" href="https://blog.nobugware.com/categories/devops">Devops</a>
                            
                        </p>
                    </header>

                    <div class="post-description">
                        

<h2 id="context">Context</h2>

<p>I wanted to blog about this for years: how to connect to a Kubernete&rsquo;s loadbalanced service?<br />
How to deal with disconnections/re-connections, maintenance?  What about gRPC specifically?<br />
The answer is heavily connected to the network stack used by Kubernetes, but with the &ldquo;Mesh Network&rdquo; revolution, It&rsquo;s not always clear how it works anymore and what the options are.
<br>
<br></p>

<h2 id="how-it-works">How it works</h2>

<p>First I recommend you to watch this great yet simple video: <a href="https://www.youtube.com/watch?v=6v_BDHIgOY8&amp;feature=youtu.be">Container Networking From Scratch</a>, then the <a href="https://kubernetes.io/docs/concepts/services-networking/service/#proxy-mode-iptables">Services clusterIP documentation</a>.</p>

<p>To make it simple when you create a Service in Kubernetes, it creates a layer 4 proxy and load balance connections to your pods using iptables, the service endpoint is one IP and a port hiding your real pods.</p>

<h2 id="the-problem">The Problem</h2>

<p>A simple TCP load balancer is good enough for a lot of things especially for HTTP/1.1 since connections are mainly short lived, the clients will try to reconnect often, so it won&rsquo;t stay connected to an old running pod.</p>

<p>But with gRPC over HTTP/2, a TCP connection is maintained open which could lead to issues, like staying connected to a dying pod or unbalancing the cluster because the clients will end on the older pods.</p>

<p>One solution is to use a more advanced proxy that knows about the higher layers.</p>

<p><a href="https://www.envoyproxy.io/">Envoy</a>, <a href="http://www.haproxy.org/">HAProxy</a> and <a href="https://traefik.io/">Traefik</a> are layer 7 reverse proxy load balancers, they know about HTTP/2 (even about gRPC) and can disconnect a backend’s pod without the clients noticing.</p>

<h2 id="edge">Edge</h2>

<p>On the edge of your Kubernetes cluster, you need a public IP, provided by your cloud provider via <a href="https://kubernetes.io/docs/concepts/services-networking/ingress/">the <code>Ingress</code> directive</a> it will expose your internal service.</p>

<p>To further control your request routing you need <a href="https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/">an Ingress Controller</a>.<br />
It&rsquo;s a reverse proxy that knows about the Kubernetes clusters and can direct the requests to the right place.
<a href="https://www.envoyproxy.io/">Envoy</a>, <a href="http://www.haproxy.org/">HAProxy</a> and <a href="https://traefik.io/">Traefik</a> can act as Ingress Controllers.</p>

<h2 id="internal-services-service-mesh">Internal Services &amp; Service Mesh</h2>

<p>In a Micro-services environment, most if not all your micro-services will also be clients to others micro-services.</p>

<p><a href="https://istio.io/">Istio</a>, a &ldquo;Mesh Network&rdquo; solution, use <a href="https://www.envoyproxy.io/">Envoy</a> as a <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/">sidecar</a>. This sidecar is configured from a central place (control plane) and makes each micro-service talking to each other through Envoy.</p>

<p>This way the client does not need to know about all the topology.</p>

<p>That&rsquo;s great but in a controlled environment (yours), where you control all the clients, sending all the traffic through a proxy is not always necessary.</p>

<h2 id="client-load-balancing">Client Load Balancing</h2>

<p>In Kubernetes you can create a <a href="https://kubernetes.io/docs/concepts/services-networking/service/#headless-services">headless service</a>; where there are no load balanced single endpoints anymore, the service pods are directly exposed, Kubernetes DNS will return all of them.</p>

<p>Here is an example service called <code>geoipd</code> scaled to 3.</p>

<pre><code>Name:      geoipd
Address 1: 172.17.0.18 172-17-0-18.geoipd.default.svc.cluster.local
Address 2: 172.17.0.21 172-17-0-21.geoipd.default.svc.cluster.local
Address 3: 172.17.0.9 172-17-0-9.geoipd.default.svc.cluster.local
</code></pre>

<p>It&rsquo;s up to your client to connect them all and load balance the connections.</p>

<p>In Go gRPC client side, a simple <code>dns:///</code> notation will fetch the entries for you, then the <a href="https://godoc.org/github.com/grpc/grpc-go/balancer/roundrobin">roundrobin package</a> will handle load balancing.</p>

<pre><code class="language-Go">conn, err := grpc.Dial(
    &quot;dns:///geoip:9200&quot;,
    grpc.WithBalancerName(roundrobin.Name),
)
</code></pre>

<p>This may sound like a good solution but it is not: the default refresh frequency is 30 minutes, meaning whenever you add new pods, it can take up to 30 minutes for them to start getting traffic!
You can lower this problem by tweaking <code>MaxConnectionAge</code> on the gRPC server:</p>

<pre><code class="language-Go">gsrv := grpc.NewServer(
    // MaxConnectionAge is just to avoid long connection, to facilitate load balancing
    // MaxConnectionAgeGrace will torn them, default to infinity
    grpc.KeepaliveParams(keepalive.ServerParameters{MaxConnectionAge: 2 * time.Minute}),
)
</code></pre>

<p>Even if you could refresh the list more often you wouldn&rsquo;t know about pod eviction fast enough and you’d miss some traffic.</p>

<p>There is a nicer solution, implementing the gRPC client resolver for Kubernetes, talking to the Kubernetes API to get the endpoints and watch them constantly, this is exactly what <a href="https://github.com/sercand/kuberesolver">Kuberesolver</a> does.</p>

<pre><code class="language-Go">// Register kuberesolver to grpc
kuberesolver.RegisterInCluster()

conn, err := grpc.Dial(
    &quot;kubernetes:///geoipd:9200&quot;,
    grpc.WithBalancerName(roundrobin.Name),
)
</code></pre>

<p>By using <code>kubernetes</code> schema you tell kuberesolver to fetch and watch the endpoints for the <code>geoipd</code> service.</p>

<p>For this to work the pod must have <code>GET</code> and <code>WATCH</code> access to <code>endpoints</code> using a role:</p>

<pre><code>kubectl create role pod-reader-role --verb=get --verb=watch --resource=endpoints,services 
kubectl create sa pod-reader-sa 
kubectl create rolebinding pod-reader-rb --role=pod-reader-role --serviceaccount=default:pod-reader-sa 
</code></pre>

<p>Redeploy your app (the client) with the service account:</p>

<pre><code class="language-yaml">spec:
  serviceAccountName: pod-reader-sa
</code></pre>

<p>Deploy, scale up, scale down, kill your pods, your client is still sending traffic to a living pod !</p>

<p>I&rsquo;m surprised it&rsquo;s not mentioned more often, client load balancing did the job for years, the same apply inside Kubernetes environment.<br />
It is fine for small to medium projects and can deal with a lot of traffic, this will do it for many of you unless if you are Netflix sized&hellip;</p>

<h2 id="conclusion">Conclusion</h2>

<p>Load-balancing proxies are great tools, especially useful on the edge of your platform. &ldquo;Mesh Network&rdquo; solutions are nice additions to our tool set, but the cost of operating and debugging a full mesh network could be really expensive and overkill in some situations, while a client load balancing solution is simple and easy to grasp.</p>

<p>Thanks to <a href="https://github.com/prune998">Prune</a> who helped me with this post, and to <a href="https://twitter.com/robteix">Robteix</a> &amp; <a href="https://twitter.com/diligiant">diligiant</a> for reviewing.</p>

                    </div>
                </section>
                
                <h1 class="content-subhead">04 Mar 2019, 00:32</h1>
                <section class="post">
                    <header class="post-header">

                        <a href="https://blog.nobugware.com/post/2019/traefik_load_balancing_grpc_services_trace_propagation/" class="post-title">Traefik gRPC Load Balancing and Traces Propagation</a>

                        <p class="post-meta">
                            
                            
                                under 
                                
                                <a class="post-category post-category-Devops" href="https://blog.nobugware.com/categories/devops">Devops</a>
                            
                        </p>
                    </header>

                    <div class="post-description">
                        

<p>Following my recent <a href="https://blog.nobugware.com/post/2019/kubernetes_quick_development_setup_minikube_prometheus_grafana/">blog post on setting up a dev environment in Kubernetes</a>, here are some tips to use <a href="https://traefik.io">Traefik</a> as a gRPC load balancer.</p>

<p><a href="https://traefik.io">Traefik</a> can be used on the edge and route incoming HTTP traffic to your Kubernetes cluster, but it&rsquo;s also <a href="https://docs.traefik.io/user-guide/grpc/">supporting gRPC</a>.<br />
<br/>
<br/>
<br/></p>

<h2 id="grpc-load-balancing-with-traefik">gRPC Load Balancing with Traefik</h2>

<p>Here I have a gRPC service I want to expose on the edge.</p>

<pre><code class="language-yaml">apiVersion: v1
kind: Service
metadata:
  name: myapp
  labels:
    name: &quot;myapp&quot;
    type: &quot;grpc&quot;
spec:
  ports:
    - port: 9200
      name: &quot;grpc&quot;
      targetPort: grpc
      protocol: TCP
  selector:
    app: &quot;myapp&quot;
  clusterIP: None
</code></pre>

<p>Note the <code>clusterIP: None</code>, it&rsquo;s a <a href="https://kubernetes.io/docs/concepts/services-networking/service/#headless-services">headless service</a>.</p>

<p>It will create a non loadbalanced service, pod&rsquo;s services can be accessed directly.</p>

<pre><code>myapp.default.svc.cluster.local.    2       IN      A       172.17.0.19
myapp.default.svc.cluster.local.    2       IN      A       172.17.0.10
myapp.default.svc.cluster.local.    2       IN      A       172.17.0.16
</code></pre>

<p>Here is the ingress for Traefik.</p>

<pre><code class="language-yaml">apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: myapp-ingress
  namespace: default
  labels:
    name: &quot;myapp&quot;
  annotations:
    ingress.kubernetes.io/protocol: h2c
spec:
  rules:
  - host: myapp-lb.minikube
    http:
      paths:
      - path: /
        backend:
          serviceName: myapp
          servicePort: 9200
</code></pre>

<p>Note the <code>h2c</code> prefix, indicating HTTP2 protocol without TLS to your backend !</p>

<p><img src="https://blog.nobugware.com/img/grpctraefik.jpg" alt="Traefik" /></p>

<h2 id="tracing">Tracing</h2>

<p>Traefik can be <a href="https://docs.traefik.io/configuration/tracing/">configured to emit tracing</a>.</p>

<p>I&rsquo;m using <a href="https://medium.com/@rghetia/distributed-tracing-and-monitoring-using-opencensus-fe5f6e9479fb"><code>ocgrpc</code> Opencensus</a>, for gRPC metrics &amp; traces.<br />
It automatically emits several counters for gRPC and traces using the <a href="https://godoc.org/google.golang.org/grpc#StatsHandler">StatsHandler</a>.</p>

<p>Unfortunately <a href="https://github.com/census-instrumentation/opencensus-go/issues/838"><code>ocgrpc</code> does not yet propagate Jaeger traces</a>, I&rsquo;ve <a href="https://github.com/akhenakh/ocgrpc_propagation">temporary forked it to support Jaeger</a>.</p>

<p>As you can see you can follow the request from Traefik down to your services.</p>

<p><img src="https://blog.nobugware.com/img/jaegergrpc.jpg" alt="Jaeger" /></p>

<p>Happy tracing !</p>

                    </div>
                </section>
                
                <h1 class="content-subhead">21 Feb 2019, 00:19</h1>
                <section class="post">
                    <header class="post-header">

                        <a href="https://blog.nobugware.com/post/2019/kubernetes_quick_development_setup_minikube_prometheus_grafana/" class="post-title">Kubernetes Quick Setup with Prometheus, Grafana &amp; Jaeger</a>

                        <p class="post-meta">
                            
                            
                                under 
                                
                                <a class="post-category post-category-Devops" href="https://blog.nobugware.com/categories/devops">Devops</a>
                            
                        </p>
                    </header>

                    <div class="post-description">
                        

<h2 id="introduction">Introduction</h2>

<p>When starting on a new project or prototyping on a new idea, I find myself doing the same tasks again and again.<br />
Thanks to Kubernetes it&rsquo;s possible to setup a new env from scratch really fast.</p>

<p>Here is a quick setup (mostly notes) to create a dev environment using Minikube and the workflow I&rsquo;m using with it.</p>

<p>Not knowing in advance where this future project will be hosted, I try to stay platform agnostic.<br />
<a href="https://opencensus.io">OpenCensus</a> or <a href="https://opentracing.io">OpenTracing</a> can abstract the target platform, letting you choose what tooling you want for your dev.</p>

<p>I consider some tools to be mandatory these days:</p>

<ul>
<li><a href="https://www.jaegertracing.io/">Jaeger</a> for tracing</li>
<li><a href="https://prometheus.io/">Prometheus</a> for instrumentation/metrics</li>
<li><a href="https://grafana.com">Grafana</a> to display these metrics</li>
<li>A logging solution: this is already taken care of by Kubernetes and depends on your cloud provider (StackDriver on GCP&hellip;), otherwise use another tool like ELK stack.<br />
On your dev, plain structured logs to stdout with Kubernetes dashboard or <a href="https://github.com/wercker/stern">Stern</a> should be fine.</li>
<li>A messaging system: for example <a href="https://nats.io/">NATS</a> but out of the scope of this post.</li>
</ul>

<h2 id="tools-installation">Tools Installation</h2>

<p>I find it easier to let Minikube open reserved ports, but not mandatory:</p>

<pre><code class="language-bash">minikube start --extra-config=apiserver.service-node-port-range=80-30000
</code></pre>

<p>I&rsquo;ll use <a href="https://traefik.io">Traefik</a> as Ingress to simplify access to several admin UI later.</p>

<p>I&rsquo;m not a big fan of <code>helm</code>, so here is a little trick, I&rsquo;m only using <code>helm</code> to create my deployment templates, using the charts repo as a source template, so I can commit or modify the resulting generated files. (Thanks <a href="https://twitter.com/prune998">Prune</a> for this tips).</p>

<pre><code class="language-bash">git clone git@github.com:helm/charts.git 

helm template charts/stable/traefik --name traefik --set metrics.prometheus.enabled=true --set rbac.enabled=true \
--set service.nodePorts.http=80 --set dashboard.enabled=true --set dashboard.domain=traefik-ui.minikube &gt; traefik.yaml 

helm template charts/stable/grafana --name grafana --set ingress.enabled=true \ 
--set ingress.hosts\[0\]=grafana.minikube --set persistence.enabled=true --set persistence.size=100Mi &gt; grafana.yaml 

helm template charts/stable/prometheus --name prometheus --set server.ingress.enabled=true \ 
--set server.ingress.hosts\[0\]=prometheus.minikube --set alertmanager.enabled=false \ 
--set kubeStateMetrics.enabled=false --set nodeExporter.enabled=false --set server.persistentVolume.enabled=true \
--set server.persistentVolume.size=1Gi --set pushgateway.enabled=false &gt; prometheus.yaml 
</code></pre>

<p>A lot more templates are available: NATS, Postgresql &hellip;</p>

<p>For Jaeger a <a href="https://raw.githubusercontent.com/jaegertracing/jaeger-kubernetes/master/all-in-one/jaeger-all-in-one-template.yml">development ready solution</a> exists, here is mine slightly tweaked to use an ingress:</p>

<pre><code>curl -o jaeger.yaml https://gist.githubusercontent.com/akhenakh/615686891340f5306dcbed82dd1d9d67/raw/41049afecafb05bc29de3b0d25208c784f963695/jaeger.yaml
</code></pre>

<p>Deploy to Minikube (ensure your current context is <code>minikube</code>&hellip;), if you need to work on several projects at the same time remember you can use Kubernetes namespaces, (beware some helm templates are overriding it).</p>

<pre><code class="language-bash">kubectl create -f traefik.yaml
kubectl create -f jaeger.yaml
kubectl create -f grafana.yaml
kubectl create -f prometheus.yaml
</code></pre>

<p>Again to ease my workflow I want Traefik to bind 80, edit the service and change it to <code>nodePort 80</code>.</p>

<pre><code>kubectl edit service traefik
</code></pre>

<p>Add some urls to your <code>/etc/hosts</code></p>

<pre><code class="language-bash">echo &quot;$(minikube ip) prometheus.minikube&quot; | sudo tee -a /etc/hosts 
echo &quot;$(minikube ip) grafana.minikube&quot; | sudo tee -a /etc/hosts 
echo &quot;$(minikube ip) traefik-ui.minikube&quot; | sudo tee -a /etc/hosts
echo &quot;$(minikube ip) jaeger.minikube&quot; | sudo tee -a /etc/hosts
</code></pre>

<p>Point your browser to any of these addresses and you are good to go !</p>

<h2 id="deploy-your-own-apps">Deploy your own apps</h2>

<p>First remember to always use the Kubernetes Docker:</p>

<pre><code class="language-bash">eval `minikube docker-env` 
</code></pre>

<p>My applications are reading parameters from environment, in Go, I&rsquo;m using <a href="http://github.com/namsral/flag">namsral/flag</a>, so the flag <code>-httpPort</code> is also set by the environment variable <code>HTTPPORT</code>.</p>

<p>I then use templates, where I set all my environment variables, to create my yaml deployment.</p>

<p><a href="https://gist.github.com/akhenakh/8c06186b7d524f6a60d40764b307a5d5">https://gist.github.com/akhenakh/8c06186b7d524f6a60d40764b307a5d5</a></p>

<p>So typical Makefile targets would fill the template with <code>envsubst</code>:</p>

<pre><code class="language-Makefile">.EXPORT_ALL_VARIABLES:
VERSION := $(shell git describe --always --tags)
DATE := $(shell date -u +%Y%m%d.%H%M%S)
LDFLAGS := -ldflags &quot;-X=main.version=$(VERSION)-$(DATE)&quot;
CGO_ENABLED=0
GOOS=linux
GOARCH=amd64
PROJET = mysuperproject
...

helloworld: 
	cd helloworld/cmd/helloworld &amp;&amp; go build $(LDFLAGS)

helloworld-image: helloworld
	cd helloworld/cmd/helloworld &amp;&amp; docker build -t helloworld:$(VERSION) .

helloworld-deploy: NAME=helloworld
helloworld-deploy: helloworld-image 
	cat deployment/project-apps.yaml | envsubst | kubectl apply -f - 
	cat deployment/project-services.yaml | envsubst | kubectl apply -f - 

project-undeploy:
    kubectl delete --ignore-not-found=true deployments,services,replicasets,pods --selector=appgroup=$(PROJECT)
</code></pre>

<p><code>Dockerfile</code> contains nothing but <code>FROM gcr.io/distroless/base</code> and a copy of the helloworld binary.</p>

<p>Note all this setup is <strong>only good for your dev</strong>, production deployment is another story.</p>

<p><code>make helloworld-deploy</code>, compilation, image creation and deployment is less than 2s over here!</p>

<h2 id="shell-tool">Shell tool</h2>

<p>Another useful trick when working with new tools is to have a shell available inside Kubernetes to experiment with.</p>

<p>Create an image for this purpose, and copy your tools and clients.</p>

<pre><code class="language-Dockerfile">FROM alpine:3.9
RUN apk add --no-cache curl busybox-extras tcpdump

WORKDIR /root/
COPY helloworldcli .
ENTRYPOINT [&quot;tail&quot;, &quot;-f&quot;, &quot;/dev/null&quot;]
</code></pre>

<p>And the relevant Makefile targets:</p>

<pre><code class="language-Makefile">debugtool-image: helloworldcli
	cp helloworld/cmd/helloworldcli/helloworldcli debugtool/helloworldcli
	cd debugtool &amp;&amp; docker build  -t debugtool:latest .
	rm -f debugtool/helloworldcli

debugtool-deploy: debugtool-image
	kubectl delete --ignore-not-found=true pod debugtool
	sleep 2
	kubectl run --restart=Never --image=debugtool:latest --image-pull-policy=IfNotPresent debugtool

debugtool-shell:
	kubectl exec -it debugtool -- /bin/sh 
</code></pre>

<p>By calling <code>make debugtool-shell</code>, you&rsquo;ll be connected on a shell inside Kubernetes.</p>

<h2 id="conclusion">Conclusion</h2>

<p>Hope this will help some of you, I would love to hear your own tricks and setup to develop on Minikube !</p>

                    </div>
                </section>
                
            </div>
            

            <div class="footer">
    <div class="pure-menu pure-menu-horizontal pure-menu-open">
        <ul>
            <li>Powered by <a class="hugo" href="http://hugo.spf13.com/" target="_blank">hugo</a></li>
        </ul>
    </div>
</div>
<script src="https://blog.nobugware.com/js/all.min.js"></script>
        </div>
    </div>
</div>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-1245966-1', 'auto');
ga('send', 'pageview');

</script>

</body>
</html>
